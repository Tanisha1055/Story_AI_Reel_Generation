
# üé• AI Story Video Generator: Multi-Model Reel Automation

This project implements an advanced, chained Generative AI workflow to automatically create a short video reel from a simple theme. The pipeline orchestrates four different state-of-the-art AI models‚Äîan LLM for scripting, a multi-stage image pipeline for visual assets, and a dedicated model for video generation‚Äîfollowed by a final media processing stage.

## ‚ú® Expected Deliverables Showcase

| Deliverable | Description | Location / Implementation Detail |
| :--- | :--- | :--- |
| **Workflow Architecture** | Explained in the **Workflow Design** section below. | Implemented across `main.py`, `workflow_steps/`, and `api_client.py`. |
| **Example Command(s)** | A single command runs the entire process end-to-end. | See **How to Run** section. |
| **Extensibility Notes** | Clear guidance on modifying themes or swapping models. | See **Extensibility & Customization** section. |
| **Skills Demonstrated** | **GenAI (Text, Image, Video), REST APIs, Media Processing, Workflow Automation.** | Showcased by the use of Gemini, Minimax, Flux, Seedance, Replicate API, and MoviePy. |

-----

## ‚öôÔ∏è Workflow Architecture and Design

The automation is structured as a sequential, three-stage pipeline to ensure consistency and quality across the final reel.

### 1\. The Model Chain

| Stage | Model Used | Function | Technology |
| :--- | :--- | :--- | :--- |
| **1. Plot Generation** | `google/gemini-2.5-flash` | Generates a structured **JSON storyboard** (scenes, prompts) and a social media caption. | Direct Gemini API |
| **2. Character Image** | `minimax/image-01` | Generates a high-fidelity **character image** for visual consistency across clips. | Replicate API |
| **3. Scene Stitching** | `flux-kontext-apps/multi-image-kontext-pro` | **Fuses** the character image with the environment/setting prompt to create the scene keyframe. | Replicate API |
| **4. Video Generation** | `bytedance/seedance-1-pro` | Animates the final fused keyframe into a short, dynamic video clip. | Replicate API |
| **5. Final Assembly** | `moviepy` | Downloads, concatenates, and exports the final MP4 reel. | Python (Media Processing) |

### 2\. Execution Flow

1.  **Scripting:** `main.py` calls `generate_story.py`, which uses Gemini to create `story_output.json`.
2.  **Looping:** The script iterates through each scene defined in the JSON.
3.  **Visual Chain:** For each scene, **Image $\rightarrow$ Stitching $\rightarrow$ Video** models are called sequentially via the `APIClient`.
4.  **Finalization:** All video URLs are downloaded, and `moviepy` combines them into `assets/final_reel.mp4`.

-----

## üöÄ How to Run the Project

Follow these steps to set up the environment and run the end-to-end automation.

### Prerequisites

  * **Python:** Ensure Python 3.9+ is installed.
  * **pip:** The Python package installer.
  * **API Tokens:** You will need API keys for the services used.

### Setup Steps

1.  **Clone the Repository** (If applicable):

    ```bash
    git clone [repository-url]
    cd [project-folder]
    ```

2.  **Install Dependencies:**

    ```bash
    pip install -r requirements.txt
    ```

3.  **Configure API Tokens:**

      * Create a file named **`.env`** in the project root.
      * Add your API tokens, which are used by `api_client.py` and `generate_story.py`:

    <!-- end list -->

    ```env
    # Required for all Replicate API calls (Image Stitching and Video Generation)
    REPLICATE_API_TOKEN="YOUR_REPLICATE_API_TOKEN" 

    # Required for Plot Generation and Captioning (uses google/gemini-2.5-flash)
    GEMINI_API_KEY="YOUR_GEMINI_API_KEY"
    ```

4.  **Execute the Pipeline:**

    ```bash
    python main.py
    ```

### Output File Structure

After running `main.py`, the `assets` folder will contain the following files, corresponding to the stages of development:

| Folder/File | Description | Workflow Stage |
| :--- | :--- | :--- |
| `story_output.json` | The raw structured script generated by Gemini. | Plot Generation |
| `images/` | Stores the character image and possibly scene images (fused). | Image Generation (Step 2 & 3) |
| `clips/` | Stores the individual video files downloaded from Seedance. | Video Generation (Step 4) |
| `final_reel.mp4` | The complete, stitched final video file. | Final Assembly (Step 5) |

-----

## üí° Extensibility & Customization

The design prioritizes ease of change through the centralized **`config.py`** (or `config.json`) file and abstracted function calls.

### 1\. User-Based Theme Input

To allow users to input their own story theme instead of choosing a random one:

1.  Open **`workflow_steps/story_generator.py`**.
2.  In the `generate_story_data` function, replace the theme selection:
    ```python
    # OLD: theme = random.choice(config['THEMES'])
    # NEW: 
    theme = input("Enter a theme of your choice for story generation: ")
    ```
    This change allows dynamic theme input at runtime.

### 2\. Changing AI Models

Model parameters are centralized in the configuration file, making replacements simple.

| Model Function | Configuration Key | Notes on Replacement |
| :--- | :--- | :--- |
| Plot Generation | `STORY_MODEL` | Replace with any LLM (e.g., LLaMA, Claude) that supports **structured JSON output** via its SDK/API. |
| Image Generation | `IMAGE_MODEL` / `MULTI_IMAGE_MODEL` | Replace with models like **SDXL** or **DALL-E 3**. Ensure the new model accepts a prompt and returns a URL. |
| Video Generation | `VIDEO_MODEL` | Replace with models like **Pika**, **RunwayML Gen-2**, or another I2V model. Ensure it accepts an image URL as input. |

### 3\. Full-Scale Video Generation

The current reel is constrained to a limited number of clips due to potential initial API testing constraints.

  * To generate the full-length video (based on the number of scenes generated by the LLM), simply ensure your **`REPLICATE_API_TOKEN`** is correctly configured in your **`.env`** file. The `main.py` loop will automatically process all generated scenes.
